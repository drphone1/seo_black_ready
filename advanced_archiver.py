"""
Parallel Web Archiver - Ultimate Version (Optimized for Speed & Stability)
"""

import os
import asyncio
import logging
import pandas as pd
from urllib.parse import urlparse
from rich.progress import Progress, SpinnerColumn, TimeElapsedColumn, TaskID, BarColumn, TimeRemainingColumn
from rich.logging import RichHandler
from rich.console import Console
from typing import List, Dict

# ---------------------- Configuration ----------------------
INPUT_EXCEL = r"E:\1-python\SEO-BLACKHOLE\2-project\0-scrap-website\good_output\content_results.xlsx"
OUTPUT_DIR = r"E:\1-python\SEO-BLACKHOLE\2-project\0-scrap-website\OUTPUT"
SINGLE_FILE_PATH = r"E:\1-python\SEO-BLACKHOLE\2-project\0-scrap-website\node_modules\.bin\single-file.cmd"
MAX_CONCURRENT_TASKS = 5  # Ú©Ø§Ù‡Ø´ ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø¨ÛŒØ´ØªØ±
REQUEST_TIMEOUT = 180  # Ú©Ø§Ù‡Ø´ Ø¨Ù‡ 3 Ø¯Ù‚ÛŒÙ‚Ù‡
DELAY_BETWEEN_REQUESTS = 1  # Ú©Ø§Ù‡Ø´ ØªØ§Ø®ÛŒØ± Ø¨Ù‡ 1 Ø«Ø§Ù†ÛŒÙ‡
# -----------------------------------------------------------

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
    handlers=[RichHandler(show_time=True, show_path=False, markup=False)]
)
logger = logging.getLogger("rich")
console = Console()

# ---------------------- Core Functions ----------------------
async def test_single_file():
    """ØªØ³Øª Ø§ÙˆÙ„ÛŒÙ‡ single-file"""
    try:
        test_command = f'"{SINGLE_FILE_PATH}" --version'
        proc = await asyncio.create_subprocess_shell(
            test_command,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await proc.communicate()
        if proc.returncode != 0:
            raise Exception("single-file Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª")
        return True
    except Exception as e:
        console.print(f"[bold red]Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª single-file: {str(e)}[/bold red]")
        return False

async def download_url(url: str, output_file: str, progress: Progress, worker_id: int) -> bool:
    """Ø¯Ø§Ù†Ù„ÙˆØ¯ ÛŒÚ© URL Ù…Ø´Ø®Øµ"""
    command = (
        f'"{SINGLE_FILE_PATH}" '
        f'"{url}" '
        f'"{output_file}" '
        f'--browser-headless '
        f'--browser-wait-until load'  # Ø¨Ø±Ú¯Ø´Øª Ø¨Ù‡ Ø­Ø§Ù„Øª Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ù‚Ø¨Ù„ÛŒ
    )
    
    progress.console.print(f"[Worker {worker_id}] [yellow]â³ Ø´Ø±ÙˆØ¹ Ø¯Ø§Ù†Ù„ÙˆØ¯: {url}[/yellow]")
    success = False
    
    try:
        proc = await asyncio.create_subprocess_shell(
            command,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=REQUEST_TIMEOUT)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ù†ØªÛŒØ¬Ù‡
        if proc.returncode == 0 and os.path.exists(output_file):
            size = os.path.getsize(output_file)
            if size > 0:
                success = True
                progress.console.print(
                    f"[Worker {worker_id}] [green]âœ“ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…ÙˆÙÙ‚ ({size/1024:.1f} KB): {url}[/green]"
                )
                # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„
                try:
                    with open(output_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        if len(content) > 0 and '<html' in content.lower():
                            progress.console.print(
                                f"[Worker {worker_id}] [blue]âš¡ ÙØ§ÛŒÙ„ HTML Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª[/blue]"
                            )
                        else:
                            success = False
                            progress.console.print(
                                f"[Worker {worker_id}] [red]Ã— Ù…Ø­ØªÙˆØ§ÛŒ HTML Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª[/red]"
                            )
                except Exception as e:
                    success = False
                    progress.console.print(
                        f"[Worker {worker_id}] [red]Ã— Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„: {str(e)}[/red]"
                    )
                
        if not success:
            if stderr:
                error_msg = stderr.decode()
            elif stdout:
                error_msg = stdout.decode()
            else:
                error_msg = "Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ"
            progress.console.print(f"[Worker {worker_id}] [red]Ã— Ø®Ø·Ø§: {error_msg}[/red]")
            
        return success
        
    except asyncio.TimeoutError:
        progress.console.print(f"[Worker {worker_id}] [red]Ã— ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª: {url}[/red]")
        return False
        
    except Exception as e:
        progress.console.print(f"[Worker {worker_id}] [red]Ã— Ø®Ø·Ø§ÛŒ Ø³ÛŒØ³ØªÙ…ÛŒ: {str(e)}[/red]")
        return False
        
    finally:
        if not success and os.path.exists(output_file):
            os.remove(output_file)

async def download_worker(queue: asyncio.Queue, progress: Progress, task_id: TaskID, worker_id: int) -> None:
    """Ú©Ø§Ø±Ú¯Ø± Ù…ÙˆØ§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ URLÙ‡Ø§"""
    worker_task = progress.add_task(f"[blue]Worker {worker_id}[/blue]", total=None)
    
    while True:
        try:
            batch = await queue.get()
            if not batch:
                break
                
            url = batch['url']
            progress.update(worker_task, description=f"[blue]Worker {worker_id}:[/blue] {urlparse(url).netloc}")
            
            domain = urlparse(url).netloc.replace('.', '_')
            output_file = os.path.join(OUTPUT_DIR, f"{domain}.html")
            
            success = await download_url(url, output_file, progress, worker_id)
            batch['success'] = success
            
            if success:
                progress.console.print(f"[Worker {worker_id}] âœ“ Ù¾Ø±Ø¯Ø§Ø²Ø´ {url} ØªÙ…Ø§Ù… Ø´Ø¯")
            else:
                progress.console.print(f"[Worker {worker_id}] Ã— Ù¾Ø±Ø¯Ø§Ø²Ø´ {url} Ø¨Ø§ Ø®Ø·Ø§ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯")
            
            await asyncio.sleep(DELAY_BETWEEN_REQUESTS)
            
        except asyncio.CancelledError:
            break
        finally:
            if 'batch' in locals() and batch:
                queue.task_done()
                progress.update(task_id, advance=1)

async def parallel_download(urls: List[str]) -> None:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…ÙˆØ§Ø²ÛŒ"""
    if not await test_single_file():
        return
        
    queue = asyncio.Queue()
    results = []
    
    MAX_WORKERS = 10  # Ø§ÙØ²Ø§ÛŒØ´ Ø¨Ù‡ 10 ÙˆØ±Ú©Ø±
    
    # ØªØ¨Ø¯ÛŒÙ„ URLÙ‡Ø§ Ø¨Ù‡ batch
    for url in urls:
        batch = {'url': url, 'success': False}
        results.append(batch)
        await queue.put(batch)
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù†
    for _ in range(MAX_WORKERS):
        await queue.put(None)
    
    with Progress(
        SpinnerColumn(),
        "[progress.description]{task.description}",
        BarColumn(),
        "[progress.percentage]{task.percentage:>3.0f}%",
        TimeElapsedColumn(),
        TimeRemainingColumn(),
        console=console,
        expand=True,
        refresh_per_second=10  # Ø§ÙØ²Ø§ÛŒØ´ Ù†Ø±Ø® Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ
    ) as progress:
        total_task = progress.add_task(
            "[bold magenta]âš¡ Ù¾ÛŒØ´Ø±ÙØª Ú©Ù„ÛŒ[/bold magenta]",
            total=len(urls)
        )
        
        workers = [
            asyncio.create_task(
                download_worker(queue, progress, total_task, i+1)
            ) 
            for i in range(MAX_WORKERS)
        ]
        
        await asyncio.gather(*workers)
        
        # Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ±
        successful = sum(1 for r in results if r['success'])
        total_size = sum(os.path.getsize(os.path.join(OUTPUT_DIR, f"{urlparse(r['url']).netloc.replace('.', '_')}.html")) 
                        for r in results if r['success'])
        
        progress.console.print(f"\n[bold]ðŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ:[/bold]")
        progress.console.print(f"âœ“ ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡: {successful}")
        progress.console.print(f"Ã— ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…ÙˆÙÙ‚: {len(urls) - successful}")
        if successful > 0:
            progress.console.print(f"ðŸ’¾ Ø­Ø¬Ù… Ú©Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯: {total_size/1024/1024:.2f} MB")
        
        if len(urls) - successful > 0:
            progress.console.print("\n[yellow]âŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…ÙˆÙÙ‚:[/yellow]")
            for r in results:
                if not r['success']:
                    progress.console.print(f"â€¢ {r['url']}")

def archive_pages(urls, output_dir):
    """
    Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³ØªÛŒ Ø§Ø² URLÙ‡Ø§ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ø§Ù…Ù„ ØµÙØ­Ø§Øª Ø¨Ù‡ ØµÙˆØ±Øª HTML Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ.
    """
    import requests
    import os
    from datetime import datetime

    for url in urls:
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = os.path.join(output_dir, f"archive_{url.split('//')[-1].replace('/', '_')}_{timestamp}.html")
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(response.text)
            if os.path.exists(filename):
                console.print(f"[green]Ø¢Ø±Ø´ÛŒÙˆ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯: [yellow]{url}[/yellow] -> {filename}[/green]")
            else:
                console.print(f"[red]ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ [yellow]{url}[/yellow] Ø°Ø®ÛŒØ±Ù‡ Ù†Ø´Ø¯.[/red]")
        except Exception as e:
            console.print(f"[red]Error archiving [yellow]{url}[/yellow]: {str(e)}[/red]")

# ---------------------- Main Execution ----------------------
def main() -> None:
    """Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡"""
    try:
        df = pd.read_excel(INPUT_EXCEL)
        urls = df['url'].dropna().unique().tolist()  # Ø§ØµÙ„Ø§Ø­ Ù…ØªØ¯ Ø§Ø² dropÙ†Ø§ Ø¨Ù‡ dropna
        console.print(f"â€¢ [cyan]ØªØ¹Ø¯Ø§Ø¯ URLÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡: {len(urls)}[/cyan]")
        
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        asyncio.run(parallel_download(urls))
        archive_pages(urls, OUTPUT_DIR)
    
    except Exception as e:
        console.print(f"â€¢ [bold red]Ø®Ø·Ø§ÛŒ Ø³ÛŒØ³ØªÙ…ÛŒ: {str(e)}[/bold red]")

if __name__ == "__main__":
    main()